{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fea8d0b0",
   "metadata": {},
   "source": [
    "Data ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c1ebb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f59481d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RAG\\my_rag\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14fa3fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 text files to process\n",
      "\n",
      "Processing:machine_learning.txt\n",
      "\n",
      "Processing:python_intro.txt\n",
      "\n",
      "Total documents loaded: 2\n"
     ]
    }
   ],
   "source": [
    "# Read all docs \n",
    "def process_all_files(directory):\n",
    "    all_documents = []\n",
    "    text_dir = Path(directory) # This will normalize the path.\n",
    "\n",
    "    #find all text files recursively\n",
    "    text_files = list(text_dir.glob(\"**/*.txt\"))\n",
    "    print(f\"Found {len(text_files)} text files to process\")\n",
    "\n",
    "    for text_file in text_files:\n",
    "        print(f\"\\nProcessing:{text_file.name}\")\n",
    "        try:\n",
    "            loader = TextLoader(str(text_file))\n",
    "            documents = loader.load()\n",
    "\n",
    "            #add source info to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = text_file.name\n",
    "                doc.metadata['file_type'] = \"text\"\n",
    "\n",
    "            all_documents.extend(documents)\n",
    "        except Exception as e:\n",
    "            print(f\" Error: {e}\")\n",
    "\n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "all_text_documents = process_all_files(\"D:/RAG/MY_RAG/data/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "443bf656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\RAG\\\\MY_RAG\\\\data\\\\machine_learning.txt', 'source_file': 'machine_learning.txt', 'file_type': 'text'}, page_content='Machine Learning (ML) is a subset of Artificial Intelligence (AI) that enables computer systems to learn and improve from experience (data) without being explicitly programmed.\\n\\nInstead of writing a fixed set of instructions for every task, you feed the ML algorithm large amounts of data, and it autonomously discovers patterns and rules to make predictions or decisions. The performance of the system improves as it gets exposed to more and more data.\\n\\nHow Machine Learning Works\\n\\nThe core idea is to train an algorithm (a set of steps) on a dataset to create a model.\\n\\n1.  Data Collection and Preparation: You gather and clean a large, high-quality dataset (e.g., thousands of labeled images, financial transaction records).\\n2.  Model Training: The algorithm is fed this data. It iteratively adjusts its internal parameters (a process called \"fitting\") to minimize the difference between its predictions and the actual outcome. This is essentially the \"learning\" part.\\n3.  Evaluation: The trained model is tested on new, unseen data to ensure it can generalize what it learned and make accurate predictions in the real world.\\n4.  Prediction/Inference: Once deployed, the model uses the patterns it learned to process new input and output a prediction (e.g., classify an email as \"spam,\" or recommend a product).\\n\\nCommon Applications\\n\\nMachine Learning is everywhere in modern technology:\\n\\n Recommendation Systems: Netflix suggesting movies, Amazon suggesting products.\\n Image and Facial Recognition: Automatically tagging photos on social media.\\n Natural Language Processing (NLP): Google Translate, virtual assistants like Siri and Alexa, and text generation (like large language models).\\n Fraud Detection: Banks flagging unusual or suspicious transactions.\\n Autonomous Vehicles: Self-driving cars using ML to interpret sensor data and make real-time driving decisions.\\n\\n\\n'),\n",
       " Document(metadata={'source': 'D:\\\\RAG\\\\MY_RAG\\\\data\\\\python_intro.txt', 'source_file': 'python_intro.txt', 'file_type': 'text'}, page_content=' Python is a high-level, general-purpose, and interpreted programming language renowned for its simplicity and code readability. Its design philosophy, encapsulated in the \"Zen of Python,\" emphasizes a clear and logical approach to programming, making it an excellent choice for both beginners and experienced developers.\\n\\n\\n\\n Key Features of Python\\n\\nHigh-Level: Python is closer to human language than machine code, meaning programmers don\\'t need to worry about low-level system details like memory management or architecture.\\nInterpreted: Python code is executed directly, line by line, by an interpreter. This speeds up the edit-test-debug cycle, as there is no separate compilation step.\\nDynamically Typed: You don\\'t need to declare the type of a variable (like integer or string) when you write the code; Python determines the type at runtime.\\nObject-Oriented: Python fully supports Object-Oriented Programming (OOP), where everything is treated as an object, but it also supports other paradigms like procedural and functional programming.\\nExtensive Standard Library and Ecosystem: Python has a massive collection of modules (pre-written code) that extend its capabilities, along with a vast ecosystem of third-party packages, making it incredibly versatile.\\nReadability: It uses significant indentation (whitespace) to define code blocks instead of curly brackets, which enforces a clean and consistent coding style.\\nOpen Source: Python is free to use and distribute, even for commercial purposes, and has a large, active community contributing to its development.\\n\\n\\n\\nCommon Use Cases\\n\\nPython\\'s versatility allows it to be used across many different domains:\\n\\nData Science and Machine Learning (ML): With libraries like NumPy, Pandas, SciPy, TensorFlow, and PyTorch, Python is the dominant language for data analysis, statistical modeling, and building ML/AI applications.\\nWeb Development: Used for server-side (backend) web development with popular frameworks like Django and Flask. Large sites like Instagram and YouTube use Python in their backend infrastructure.\\nScripting and Automation: Often used as a scripting language to automate repetitive tasks, such as renaming files, sending emails, processing text, or managing system operations.\\nSoftware Development: It\\'s used for prototyping, creating software, and acting as a \"glue language\" to integrate components written in other languages.\\nFinance and Scientific Computing: Its powerful libraries make it suitable for complex calculations, simulations, and financial modeling.\\n\\nPython was created in the late 1980s by Guido van Rossum, who named it after the BBC comedy show onty Python\\'s Flying Circus\\n')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c88a67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chucnk_data(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"split {len(documents)} documents into {len(split_docs)} chuncks\")\n",
    "\n",
    "    #show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\n Example chunks:\")\n",
    "        print(f\"content:{split_docs[0].page_content[:200]} ...\")\n",
    "        print(f\"metadata: {split_docs[0].metadata}\")\n",
    "    return split_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "076e42f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split 2 documents into 8 chuncks\n",
      "\n",
      " Example chunks:\n",
      "content:Machine Learning (ML) is a subset of Artificial Intelligence (AI) that enables computer systems to learn and improve from experience (data) without being explicitly programmed.\n",
      "\n",
      "Instead of writing a f ...\n",
      "metadata: {'source': 'D:\\\\RAG\\\\MY_RAG\\\\data\\\\machine_learning.txt', 'source_file': 'machine_learning.txt', 'file_type': 'text'}\n"
     ]
    }
   ],
   "source": [
    "chuncks = chucnk_data(all_text_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adeca030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingsManager at 0x21054e030b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingsManager:\n",
    "    \"\"\"\n",
    "    Manages loading of embedding models and generating embeddings.\n",
    "    Args:\n",
    "        model_name (str): Name of the embedding model to load.   \n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()} \")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise e\n",
    "        \n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts.\n",
    "\n",
    "        Args:\n",
    "            texts (List[str]): List of input texts.\n",
    "\n",
    "        Returns:   \n",
    "            np.ndarray: Array of embeddings.\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded. Call load_model() first.\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "\n",
    "emb_manager = EmbeddingsManager()\n",
    "emb_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f360bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB collection 'documents' initialized at 'D:\\\\RAG\\\\my_rag\\\\data\\\\vector_store'\n",
      "Current number of documents in collection: 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x21054e00f20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"\n",
    "    Manages a vector store using ChromaDB for storing and retrieving document embeddings.\n",
    "    \n",
    "    Args:\n",
    "        collection_name (str): Name of the ChromaDB collection.\n",
    "        persist_directory (str): Directory to persist the ChromaDB database.\n",
    "    \"\"\"\n",
    "    def __init__(self, collection_name: str = \"documents\", persist_directory: str = r\"D:\\\\RAG\\\\my_rag\\\\data\\\\vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the VectorStore with ChromaDB client and collection.\n",
    "        Args:\n",
    "            collection_name (str): Name of the ChromaDB collection.\n",
    "            persist_directory (str): Directory to persist the ChromaDB database.\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"\n",
    "        Initialize the ChromaDB client and collection.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            #create persist chromaDB \n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"text documents embeddings for RAG\"}\n",
    "                )\n",
    "            print(f\"ChromaDB collection '{self.collection_name}' initialized at '{self.persist_directory}'\")\n",
    "            print(f\"Current number of documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing ChromaDB: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        add documents and their embeddings to the vector store.\n",
    "\n",
    "        args:\n",
    "            documents (List[Any]): List of document objects with metadata.\n",
    "            embeddings (np.ndarray): Array of embeddings corresponding to the documents.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(documents) != embeddings.shape[0]:\n",
    "            raise ValueError(\"Number of documents and embeddings must match.\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to the vector store...\")\n",
    "\n",
    "        # Prepare data for insertion for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_texts = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embeddings) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID for each document\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            # Prepare metadata and text\n",
    "            metadata = dict(doc.metadata)  # Ensure metadata is a dictionary\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_texts.append(doc.page_content)\n",
    "\n",
    "            # Embedding\n",
    "            embeddings_list.append(embeddings.tolist())\n",
    "\n",
    "        # print(f\"metadata count: {len(metadatas)}\")\n",
    "        # Add to collection chromaDB\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_texts,\n",
    "                embeddings=embeddings_list\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to the vector store.\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise e\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8af175ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 8 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (8, 384)\n",
      "Adding 8 documents to the vector store...\n",
      "Successfully added 8 documents to the vector store.\n",
      "Total documents in collection: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### convert chuncks to embeddings\n",
    "\n",
    "## Extract texts from chuncks\n",
    "texts =[doc.page_content for doc in chuncks]\n",
    "\n",
    "## Generate embeddings\n",
    "embeddings = emb_manager.generate_embeddings(texts)\n",
    "\n",
    "## Add documents and embeddings to vector store\n",
    "vectorstore.add_documents(chuncks, embeddings) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986a5d63",
   "metadata": {},
   "source": [
    "Retriever Pipeline From VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b43c9f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"\n",
    "    Retriever class to fetch relevant documents from the vector store based on query embeddings.\n",
    "    \n",
    "    Args:\n",
    "        vector_store (VectorStore): Instance of the VectorStore class.\n",
    "        emb_manager (EmbeddingsManager): Instance of the EmbeddingsManager class.\n",
    "        top_k (int): Number of top similar documents to retrieve.\n",
    "    \"\"\"\n",
    "    def __init__(self, vector_store: VectorStore, emb_manager: EmbeddingsManager):\n",
    "        self.vector_store = vector_store\n",
    "        self.emb_manager = emb_manager\n",
    "\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve top_k similar documents from the vector store based on the query.\n",
    "\n",
    "        Args:\n",
    "            query (str): Input query string.\n",
    "            top_k (int): Number of top similar documents to retrieve.\n",
    "            score_threshold (float): Minimum similarity score threshold for retrieval.\n",
    "        Returns:   \n",
    "            List[Dict[str, Any]]: List of retrieved documents with metadata.\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Using top_k={top_k} and score_threshold={score_threshold}\")\n",
    "\n",
    "\n",
    "        print(f\"Generating embedding for the query: '{query}'\")\n",
    "        query_embedding = self.emb_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        # Search in vector store\n",
    "        try:\n",
    "            print(f\"Searching for top {top_k} similar documents in the vector store...\")\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "\n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                print(f\"Found {len(results['documents'][0])} documents.\")\n",
    "                document = results['documents'][0]\n",
    "                metadata = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, document, metadata, distances)):\n",
    "                    similarity_score = 1 - distance  # Convert distance to similarity score\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            \"id\": doc_id,\n",
    "                            \"content\": document,\n",
    "                            \"metadata\": metadata,\n",
    "                            \"similarity_score\": similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "\n",
    "            print(f\"Retrieved {len(retrieved_docs)} documents.\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            raise e\n",
    "\n",
    "rag_retriever = RAGRetriever(vector_store=vectorstore, emb_manager=emb_manager)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc0e66",
   "metadata": {},
   "source": [
    "# LLM Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e202ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini API initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "def configure_gemini():\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "    if not api_key:\n",
    "        raise ValueError(\"GOOGLE_API_KEY is missing in your .env file.\")\n",
    "\n",
    "    genai.configure(api_key=api_key)\n",
    "    print(\"Gemini API initialized successfully.\")\n",
    "\n",
    "configure_gemini()\n",
    "\n",
    "class RAGGenerator:\n",
    "    \"\"\"\n",
    "    Generates answers based on retrieved document chunks using a generative model.\n",
    "    Args:\n",
    "        model_name (str): Name of the generative model to use.\n",
    "    returns:\n",
    "        str: Generated answer.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"gemini-2.5-flash\"):\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Initialize the generative AI client\n",
    "        # self.client = generativeai.Client()  --- IGNORE ---\n",
    "\n",
    "    def generate_answer(self, question: str, retrieved_chunks: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"\n",
    "        Generate an answer to the question based on the retrieved document chunks.\n",
    "\n",
    "        Args:\n",
    "            question (str): The user's question.\n",
    "            retrieved_chunks (List[Dict[str, Any]]): List of retrieved document chunks.\n",
    "\n",
    "        Returns:\n",
    "            str: Generated answer.\n",
    "        \"\"\"\n",
    "        # Convert chunks into a single context block\n",
    "        context_text = \"\\n\\n\".join(\n",
    "            [f\"Source {i+1}:\\n{chunk['content']}\" for i, chunk in enumerate(retrieved_chunks)]\n",
    "        )\n",
    "         \n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert AI assistant.\n",
    "\n",
    "Use ONLY the following retrieved context to answer the user's question.\n",
    "if you do not know the answer, say \"The information is not available in the provided documents.\"\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "User Question: {question}\n",
    "\n",
    "Give a clear, helpful answer. Do NOT hallucinate. If the answer is not in the context, say \"The information is not available in the provided documents.\"\n",
    "\"\"\"\n",
    "        model = genai.GenerativeModel(self.model_name)\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    \n",
    "\n",
    "rag_generator = RAGGenerator()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ab263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60de4dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_call(question: str):\n",
    "    retrieved_chunks = rag_retriever.retrieve(query=question, top_k=5, score_threshold=0.1)\n",
    "    if not retrieved_chunks:\n",
    "        raise ValueError(\"No relevant documents found for the query.\")\n",
    "    \n",
    "    answer = rag_generator.generate_answer(\n",
    "        question=question,\n",
    "        retrieved_chunks=retrieved_chunks\n",
    "    )\n",
    "    if not answer or answer.strip() == \"\":\n",
    "        raise ValueError(\"Unable to generate an answer.\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0718a3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'what is python?'\n",
      "Using top_k=5 and score_threshold=0.1\n",
      "Generating embedding for the query: 'what is python?'\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 44.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Searching for top 5 similar documents in the vector store...\n",
      "Found 5 documents.\n",
      "Retrieved 5 documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Python is a high-level, general-purpose, and interpreted programming language renowned for its simplicity and code readability. Its design philosophy, encapsulated in the \"Zen of Python,\" emphasizes a clear and logical approach to programming, making it an excellent choice for both beginners and experienced developers.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_call(\"what is python?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72c1171",
   "metadata": {},
   "source": [
    "# Project Report – Retrieval-Augmented Generation (RAG) System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a0023",
   "metadata": {},
   "source": [
    "This project implements a Retrieval-Augmented Generation (RAG) pipeline designed to demonstrate the ability to use LangChain, vector databases, embedding models, and modern LLMs to build a grounded question-answering system. The approach was centered around creating a modular, readable, and complete workflow that ingests documents, processes them into embeddings, retrieves semantically relevant context, and synthesizes an answer using a generative model.\n",
    "\n",
    "The pipeline begins by loading .txt documents using LangChain’s TextLoader, followed by chunking through RecursiveCharacterTextSplitter. Chunking ensures that the retrieval step works on smaller, semantically meaningful units of text. SentenceTransformer embeddings (all-MiniLM-L6-v2) were selected for their balance of speed and quality, and the resulting vectors were stored in a ChromaDB persistent collection. A custom retriever component handles query encoding and similarity search, and the final answer is produced by Google’s Gemini Flash 2.5 model using a context-restricted prompt that prevents hallucination and enforces source dependence.\n",
    "\n",
    "Several trade-offs were made in the design. SentenceTransformer was chosen instead of LangChain’s built-in embedding abstractions to maintain full control and efficiency, though this reduces out-of-the-box LangChain integration. ChromaDB was selected as the vector store for simplicity and local persistence, rather than using Pinecone, Weaviate, or Qdrant, which might offer greater scalability. Gemini Flash 2.5 was used because of its strong instruction following and low latency, but other models (e.g., OpenAI or Cohere) might offer improved reasoning. Additionally, the system supports only .txt documents, and retrieval uses a simple cosine similarity approach rather than hybrid or reranked search.\n",
    "\n",
    "Future improvements include extending the document loader to support PDFs, websites, and structured data formats. The retrieval quality can be enhanced using cross-encoder reranking, hybrid BM25+vector search, or chunk-metadata-based filtering. The system can be expanded to a full application by adding a Streamlit or FastAPI interface and implementing chat history or session memory. Finally, evaluation metrics could be added for measuring retrieval precision and grounding quality across different document sets. These enhancements would make the system more robust, scalable, and applicable to real-world use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ed9f85",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
