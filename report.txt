This project implements a Retrieval-Augmented Generation (RAG) pipeline designed to demonstrate the ability to use LangChain, vector databases, embedding models, and modern LLMs to build a grounded question-answering system. The approach was centered around creating a modular, readable, and complete workflow that ingests documents, processes them into embeddings, retrieves semantically relevant context, and synthesizes an answer using a generative model.

The pipeline begins by loading .txt documents using LangChain’s TextLoader, followed by chunking through RecursiveCharacterTextSplitter. Chunking ensures that the retrieval step works on smaller, semantically meaningful units of text. SentenceTransformer embeddings (all-MiniLM-L6-v2) were selected for their balance of speed and quality, and the resulting vectors were stored in a ChromaDB persistent collection. A custom retriever component handles query encoding and similarity search, and the final answer is produced by Google’s Gemini Flash 2.5 model using a context-restricted prompt that prevents hallucination and enforces source dependence.

Several trade-offs were made in the design. SentenceTransformer was chosen instead of LangChain’s built-in embedding abstractions to maintain full control and efficiency, though this reduces out-of-the-box LangChain integration. ChromaDB was selected as the vector store for simplicity and local persistence, rather than using Pinecone, Weaviate, or Qdrant, which might offer greater scalability. Gemini Flash 2.5 was used because of its strong instruction following and low latency, but other models (e.g., OpenAI or Cohere) might offer improved reasoning. Additionally, the system supports only .txt documents, and retrieval uses a simple cosine similarity approach rather than hybrid or reranked search.

Future improvements include extending the document loader to support PDFs, websites, and structured data formats. The retrieval quality can be enhanced using cross-encoder reranking, hybrid BM25+vector search, or chunk-metadata-based filtering. The system can be expanded to a full application by adding a Streamlit or FastAPI interface and implementing chat history or session memory. Finally, evaluation metrics could be added for measuring retrieval precision and grounding quality across different document sets. These enhancements would make the system more robust, scalable, and applicable to real-world use cases.